{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, UpSampling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "TensorFlow is an open source machine learning framework developed by Google that is widely used in building and training deep neural networks.\n",
    "\n",
    "MNIST is a dataset of handwritten digits that is commonly used for image classification tasks. It contains 60,000 training images and 10,000 test images.\n",
    "\n",
    "Sequential and Model are two different ways to define a neural network architecture in Keras. Sequential is used for building a linear stack of layers, while Model allows for more complex network architectures with multiple inputs and outputs.\n",
    "\n",
    "These are various types of layers that can be used to build a neural network architecture in Keras.\n",
    "\n",
    "Conv2D is a 2D convolution layer that performs a convolution operation on the input data.\n",
    "\n",
    "MaxPooling2D is a 2D pooling layer that performs a max pooling operation on the input data.\n",
    "\n",
    "Flatten is a layer that flattens the output of the previous layer into a 1D vector.\n",
    "\n",
    "Dense is a fully connected layer that connects every input neuron to every output neuron.\n",
    "\n",
    "Dropout is a regularization technique that randomly drops out some of the neurons during training to prevent overfitting.\n",
    "\n",
    "Input is a layer that specifies the shape of the input data.\n",
    "\n",
    "UpSampling2D is a 2D upsampling layer that performs a nearest-neighbor interpolation on the input data to increase its size.\n",
    "\n",
    "to_categorical is a function in Keras that converts a class vector (integers) to a binary class matrix.\n",
    " \n",
    "NumPy is a Python library for numerical computing that is commonly used for data manipulation and scientific computing.\n",
    "\n",
    "Matplotlib is a Python library for creating visualizations such as graphs, charts, and plots. It is commonly used for data visualization in machine learning and data science.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Rescale input data to range [0,1]\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code loads the MNIST dataset from the Keras library. The dataset is split into two parts: a training set (x_train, y_train) and a test set (x_test, y_test).\n",
    "The input data from the range of [0,255] to the range of [0,1]. This is done to normalize the data and make it easier for the neural network to learn from. The astype() method is used to convert the data type of the input data from int to float32.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 28, 28, 1)\n",
      "Testing set shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input data to 4D tensor with shape (batch_size, height, width, channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Print dataset information\n",
    "print('Training set shape:', x_train.shape)\n",
    "print('Testing set shape:', x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code reshapes the input data from a 3D tensor with shape (batch_size, height, width) to a 4D tensor with shape (batch_size, height, width, channels). In this case, the channels dimension is set to 1 because the input images are grayscale. If the images were RGB, the channels dimension would be set to 3.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder model\n",
    "input_img = Input(shape=(28,28,1))\n",
    "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2,2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(128, (3,3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(32, (3,3), activation='relu')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "decoded = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code builds an autoencoder model using the Keras library. The model has an input layer of shape (28, 28, 1), which corresponds to the shape of the input images. The model consists of three convolutional layers with 32, 64, and 128 filters respectively. Each convolutional layer is followed by a max pooling layer. The output of the final max pooling layer is the encoded representation of the input images. The encoded representation is then decoded by three convolutional layers with 128, 64, and 32 filters respectively. Each convolutional layer is followed by an upsampling layer. The output of the final upsampling layer is the decoded images. The activation function used for the convolutional layers is ReLU, and the activation function used for the output layer is sigmoid.\n",
    "\n",
    "Note that this autoencoder model is designed to compress the input images into a lower dimensional representation and then reconstruct them from the compressed representation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code creates an instance of the autoencoder model by specifying the input and output layers using the Model class from Keras. The input layer is defined as input_img, which is of shape (28, 28, 1). The output layer is defined as decoded, which is the result of the last convolutional layer.\n",
    "The compile method is used to configure the model for training. In this case, the optimizer used is adam, which is an algorithm for stochastic optimization. The loss function used is binary_crossentropy, which is a popular choice for binary classification problems, where the goal is to minimize the difference between the predicted output and the true output.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 27s 13ms/step - loss: 0.0948 - val_loss: 0.0749\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0737 - val_loss: 0.0718\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0709 - val_loss: 0.0697\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0694 - val_loss: 0.0690\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0685 - val_loss: 0.0678\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0678 - val_loss: 0.0672\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0673 - val_loss: 0.0666\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0669 - val_loss: 0.0671\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0666 - val_loss: 0.0661\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0663 - val_loss: 0.0663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23283d09180>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train autoencoder model\n",
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=32, validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code trains the autoencoder model using the fit method from Keras. The input and output data are both x_train, which are the preprocessed images of the MNIST dataset. The epochs parameter specifies the number of times to iterate over the entire training dataset. The batch_size parameter specifies the number of samples to use in each batch of training.\n",
    "\n",
    "The validation_data parameter specifies the data on which to evaluate the loss and any model metrics at the end of each epoch. In this case, the validation data is set to x_test, which is the preprocessed test set of the MNIST dataset.\n",
    "\n",
    "During training, the model learns to minimize the binary cross-entropy loss between the predicted output and the true output. The model parameters are updated using the adam optimizer specified during the compilation step. After training is complete, the trained autoencoder model can be used to reconstruct images or for other downstream tasks such as feature extraction.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract features from input data using the trained autoencoder\n",
    "encoder = Model(input_img, encoded)\n",
    "x_train_features = encoder.predict(x_train)\n",
    "x_test_features = encoder.predict(x_test)\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code extracts features from the input data using the trained autoencoder model. The encoder model is created by specifying the input layer as input_img and the output layer as encoded. This model maps the input images to the encoded features learned by the autoencoder.\n",
    "\n",
    "The predict method is then used to generate encoded features for the training and test datasets. These encoded features can be used for downstream tasks such as classification or clustering.\n",
    "\n",
    "Finally, the class labels y_train and y_test are converted to one-hot encoded vectors using the to_categorical method from Keras. This is often required for classification tasks where the class labels are represented as integers but need to be converted to a vector representation for training machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(4,4,128))) # Input shape is (batch_size, height, width, channels) after feature extraction\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, activation='softmax'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code builds a CNN model for classification. The input to the model is the encoded features extracted using the autoencoder, which have a shape of (batch_size, height, width, channels) after feature extraction.\n",
    "\n",
    "The Flatten layer is used to flatten the encoded features into a 1D vector so that they can be fed into the fully connected layers. The Dense layer with 128 units and ReLU activation function is added after the Flatten layer, followed by a Dropout layer with a dropout rate of 0.5 to prevent overfitting.\n",
    "\n",
    "Finally, the output layer is a Dense layer with 10 units (since there are 10 classes in the MNIST dataset) and a softmax activation function, which is used to produce the class probabilities for each input image.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code compiles the CNN model with the Adam optimizer and categorical cross-entropy loss function, which is commonly used for multi-class classification problems. The accuracy metric is also specified to track the performance of the model during training and evaluation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 32s 9ms/step - loss: 0.2943 - accuracy: 0.9111 - val_loss: 0.1002 - val_accuracy: 0.9673\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1533 - accuracy: 0.9535 - val_loss: 0.0633 - val_accuracy: 0.9796\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1332 - accuracy: 0.9603 - val_loss: 0.0502 - val_accuracy: 0.9834\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1218 - accuracy: 0.9613 - val_loss: 0.0499 - val_accuracy: 0.9833\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1113 - accuracy: 0.9663 - val_loss: 0.0500 - val_accuracy: 0.9837\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1057 - accuracy: 0.9669 - val_loss: 0.0471 - val_accuracy: 0.9848\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0986 - accuracy: 0.9692 - val_loss: 0.0455 - val_accuracy: 0.9859\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0928 - accuracy: 0.9705 - val_loss: 0.0357 - val_accuracy: 0.9889\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0879 - accuracy: 0.9740 - val_loss: 0.0416 - val_accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0861 - accuracy: 0.9730 - val_loss: 0.0403 - val_accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train_features, y_train, epochs=10, batch_size=32, validation_data=(x_test_features, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code trains the compiled CNN model using the extracted features from the trained autoencoder (x_train_features and x_test_features) as input data and the one-hot encoded class labels (y_train and y_test) as target data. The model is trained for 10 epochs with a batch size of 32 and the validation data is used to evaluate the model's performance after each epoch. The training history is stored in the history object.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training and testing accuracy\n",
    "train_acc = history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "The variable train_acc is not assigned any value in the code you provided. To print the training and testing accuracy, you can use the history object that was created during training\n",
    "This code uses the history object to extract the final training and testing accuracy from the training process. The history.history attribute is a dictionary that contains the loss and metrics values recorded during training. The 'accuracy' key corresponds to the training accuracy and the 'val_accuracy' key corresponds to the testing accuracy. The [-1] index is used to extract the last value recorded for each metric. The f-string is used to print the accuracy values with 4 decimal places.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 5\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "Predicted label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image:\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed image using autoencoder:\n"
     ]
    }
   ],
   "source": [
    "# Select a random image from test set and predict its label\n",
    "import random\n",
    "\n",
    "index = random.randint(0, x_test.shape[0]-1)\n",
    "img = x_test[index]\n",
    "label = y_test[index]\n",
    "print('True label:', np.argmax(label))\n",
    "\n",
    "# Reshape input data to 4D tensor with shape (1, height, width, channels)\n",
    "img = img.reshape(1, 28, 28, 1)\n",
    "\n",
    "# Extract features from input image using the trained autoencoder\n",
    "img_features = encoder.predict(img)\n",
    "\n",
    "# Predict label using the trained CNN model\n",
    "prediction = model.predict(img_features)[0]\n",
    "print('Predicted label:', np.argmax(prediction))\n",
    "\n",
    "# Display the input image\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "plt.show()\n",
    "print('Input image:')\n",
    "plt.show()\n",
    "\n",
    "# Display the reconstructed image using the trained autoencoder\n",
    "reconstructed_img = autoencoder.predict(img)\n",
    "plt.imshow(reconstructed_img[0], cmap='gray')\n",
    "plt.show()\n",
    "print('Reconstructed image using autoencoder:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RCL_D2FLT</th>\n",
       "      <th>RCLS_CHM_C</th>\n",
       "      <th>RCLS_CHM_D</th>\n",
       "      <th>RCLS_COM_C</th>\n",
       "      <th>RCLS_COM_D</th>\n",
       "      <th>RCLS_CTCT_</th>\n",
       "      <th>RCLS_D1ANT</th>\n",
       "      <th>RCLS_D1FLT</th>\n",
       "      <th>RCLS_D1XD2</th>\n",
       "      <th>RCLS_D2_X</th>\n",
       "      <th>RCLS_D2ANT</th>\n",
       "      <th>RCLS_D2FLT</th>\n",
       "      <th>RCLS_DE_D2</th>\n",
       "      <th>RCLS_DELFT</th>\n",
       "      <th>RCLS_DOL_D</th>\n",
       "      <th>RCLS_GRN</th>\n",
       "      <th>RCLS_STR_D</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RCL_D2FLT  RCLS_CHM_C  RCLS_CHM_D  RCLS_COM_C  RCLS_COM_D  RCLS_CTCT_  \\\n",
       "0          2           2           2           3           2           2   \n",
       "1          0           0           0           0           0           2   \n",
       "2          1           0           0           0           0           2   \n",
       "3          2           2           1           3           1           2   \n",
       "4          2           2           1           3           1           2   \n",
       "\n",
       "   RCLS_D1ANT  RCLS_D1FLT  RCLS_D1XD2  RCLS_D2_X  RCLS_D2ANT  RCLS_D2FLT  \\\n",
       "0           1           1           1          2           2           2   \n",
       "1           0           0           1          0           0           0   \n",
       "2           1           2           2          1           1           2   \n",
       "3           1           2           1          2           2           2   \n",
       "4           1           2           2          2           1           2   \n",
       "\n",
       "   RCLS_DE_D2  RCLS_DELFT  RCLS_DOL_D  RCLS_GRN  RCLS_STR_D  label  \n",
       "0           2           2           2         3           2      0  \n",
       "1           0           0           0         1           0      0  \n",
       "2           2           2           0         0           0      0  \n",
       "3           2           2           1         3           1      0  \n",
       "4           2           2           0         0           0      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trainingTestingDataset.csv')\n",
    "df.drop(['FID','RCLS_GEOL'],axis='columns',inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df.drop('label',axis='columns').to_numpy()\n",
    "target = df.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into training and testing sets\n",
    "features_train, features_test, y_train, y_test = train_test_split(inputs, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize data\n",
    "mean = features_train.mean(axis=0)\n",
    "std = features_train.std(axis=0)\n",
    "features_train = (features_train - mean) / std\n",
    "features_test = (features_test - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 90ms/step - loss: 0.9947 - val_loss: 0.9666\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.9628 - val_loss: 0.9337\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.9265 - val_loss: 0.8866\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.8748 - val_loss: 0.8238\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.8146 - val_loss: 0.7508\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7455 - val_loss: 0.6788\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6737 - val_loss: 0.6142\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6067 - val_loss: 0.5544\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5446 - val_loss: 0.4950\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.4809 - val_loss: 0.4476\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4318 - val_loss: 0.4107\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3936 - val_loss: 0.3752\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3583 - val_loss: 0.3465\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.3298 - val_loss: 0.3236\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3079 - val_loss: 0.3019\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2879 - val_loss: 0.2824\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2716 - val_loss: 0.2711\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2581 - val_loss: 0.2602\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2455 - val_loss: 0.2477\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.2348 - val_loss: 0.2385\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2254 - val_loss: 0.2309\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2167 - val_loss: 0.2229\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2096 - val_loss: 0.2194\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.2027 - val_loss: 0.2126\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1950 - val_loss: 0.2073\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1889 - val_loss: 0.2013\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1822 - val_loss: 0.1974\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1764 - val_loss: 0.1932\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1709 - val_loss: 0.1890\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1653 - val_loss: 0.1829\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1604 - val_loss: 0.1794\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1565 - val_loss: 0.1779\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1517 - val_loss: 0.1760\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1485 - val_loss: 0.1709\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1448 - val_loss: 0.1700\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1417 - val_loss: 0.1662\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1387 - val_loss: 0.1646\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1358 - val_loss: 0.1614\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1325 - val_loss: 0.1595\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1298 - val_loss: 0.1556\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1268 - val_loss: 0.1535\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.1238 - val_loss: 0.1516\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1213 - val_loss: 0.1475\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1184 - val_loss: 0.1456\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1158 - val_loss: 0.1416\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1141 - val_loss: 0.1386\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1120 - val_loss: 0.1366\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.1084 - val_loss: 0.1362\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1066 - val_loss: 0.1288\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1043 - val_loss: 0.1295\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1023 - val_loss: 0.1266\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0998 - val_loss: 0.1221\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0988 - val_loss: 0.1209\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0964 - val_loss: 0.1191\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0936 - val_loss: 0.1176\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0917 - val_loss: 0.1135\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0901 - val_loss: 0.1119\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0889 - val_loss: 0.1093\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0869 - val_loss: 0.1076\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0846 - val_loss: 0.1049\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0839 - val_loss: 0.1040\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0819 - val_loss: 0.1024\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0805 - val_loss: 0.1011\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0787 - val_loss: 0.0981\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0771 - val_loss: 0.0976\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0759 - val_loss: 0.0963\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0743 - val_loss: 0.0957\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0733 - val_loss: 0.0945\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0717 - val_loss: 0.0936\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0708 - val_loss: 0.0932\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0698 - val_loss: 0.0926\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0684 - val_loss: 0.0912\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0677 - val_loss: 0.0913\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0668 - val_loss: 0.0906\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0654 - val_loss: 0.0882\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0644 - val_loss: 0.0908\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0633 - val_loss: 0.0894\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0621 - val_loss: 0.0869\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0620 - val_loss: 0.0871\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0608 - val_loss: 0.0864\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0596 - val_loss: 0.0861\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0589 - val_loss: 0.0867\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0579 - val_loss: 0.0850\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0573 - val_loss: 0.0856\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0559 - val_loss: 0.0845\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0555 - val_loss: 0.0830\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0549 - val_loss: 0.0844\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0539 - val_loss: 0.0844\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0536 - val_loss: 0.0842\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0533 - val_loss: 0.0844\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0529 - val_loss: 0.0831\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0523 - val_loss: 0.0810\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0504 - val_loss: 0.0825\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0495 - val_loss: 0.0815\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0488 - val_loss: 0.0821\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0484 - val_loss: 0.0823\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0477 - val_loss: 0.0803\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0468 - val_loss: 0.0803\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0463 - val_loss: 0.0797\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0459 - val_loss: 0.0804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23191257580>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define autoencoder model\n",
    "input_shape = (features_train.shape[1],)\n",
    "input_layer = Input(shape=input_shape)\n",
    "encoder = Dense(64, activation='relu')(input_layer)\n",
    "encoder = Dense(32, activation='relu')(encoder)\n",
    "encoder = Dense(16, activation='relu')(encoder)\n",
    "decoder = Dense(32, activation='relu')(encoder)\n",
    "decoder = Dense(64, activation='relu')(decoder)\n",
    "decoder = Dense(features_train.shape[1], activation='linear')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train autoencoder model\n",
    "autoencoder.fit(features_train, features_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 60ms/step - loss: 0.5418 - accuracy: 0.8176 - val_loss: 0.5285 - val_accuracy: 0.8158\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3846 - accuracy: 0.8784 - val_loss: 0.4632 - val_accuracy: 0.8158\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3374 - accuracy: 0.8784 - val_loss: 0.3678 - val_accuracy: 0.8158\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.3027 - accuracy: 0.8784 - val_loss: 0.3286 - val_accuracy: 0.8158\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.2825 - accuracy: 0.8784 - val_loss: 0.3270 - val_accuracy: 0.8158\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2561 - accuracy: 0.8851 - val_loss: 0.2767 - val_accuracy: 0.8684\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2466 - accuracy: 0.9122 - val_loss: 0.2555 - val_accuracy: 0.8421\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2361 - accuracy: 0.9189 - val_loss: 0.2631 - val_accuracy: 0.8684\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2280 - accuracy: 0.9122 - val_loss: 0.2468 - val_accuracy: 0.8421\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.2226 - accuracy: 0.9122 - val_loss: 0.2308 - val_accuracy: 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231912ce410>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract features from autoencoder\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "features_train = encoder_model.predict(features_train)\n",
    "features_test = encoder_model.predict(features_test)\n",
    "\n",
    "# Reshape features for CNN model\n",
    "features_train = np.expand_dims(features_train, axis=2)\n",
    "features_test = np.expand_dims(features_test, axis=2)\n",
    "\n",
    "# Define CNN model\n",
    "input_shape = (features_train.shape[1], features_train.shape[2])\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv1 = Conv1D(32, 3, activation='relu', padding='same')(input_layer)\n",
    "pool1 = MaxPooling1D(2, padding='same')(conv1)\n",
    "conv2 = Conv1D(64, 3, activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling1D(2, padding='same')(conv2)\n",
    "conv3 = Conv1D(128, 3, activation='relu', padding='same')(pool2)\n",
    "pool3 = MaxPooling1D(2, padding='same')(conv3)\n",
    "flatten = Flatten()(pool3)\n",
    "output_layer = Dense(1, activation='sigmoid')(flatten)\n",
    "cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model.fit(features_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        39\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.83        47\n",
      "   macro avg       0.41      0.50      0.45        47\n",
      "weighted avg       0.69      0.83      0.75        47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\samra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\samra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Predict labels for testing data using CNN model\n",
    "# Make predictions using CNN model\n",
    "y_pred_prob = cnn_model.predict(features_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print classification reports\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
