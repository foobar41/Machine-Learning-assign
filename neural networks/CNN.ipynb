{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code block imports several modules from the TensorFlow library:\n",
    "\n",
    "tensorflow is the main module for TensorFlow.\n",
    "tensorflow.keras is a high-level API for building and training deep learning models in TensorFlow.\n",
    "tensorflow.keras.datasets is a module that provides access to common datasets for machine learning and deep learning, including the MNIST dataset.\n",
    "tensorflow.keras.models is a module that provides tools for building deep learning models, including the Sequential model.\n",
    "tensorflow.keras.layers is a module that provides a variety of layers for building deep learning models, including Conv2D, MaxPooling2D, Flatten, Dense, and Dropout.\n",
    "tensorflow.keras.utils is a module that provides utilities for working with data in TensorFlow, including to_categorical.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code block loads the MNIST dataset using the mnist.load_data() function from the tensorflow.keras.datasets module. The dataset consists of 60,000 training images and 10,000 test images of handwritten digits (0-9), each represented as a 28x28 grayscale image.\n",
    "\n",
    "The function returns two tuples: (x_train, y_train) and (x_test, y_test). x_train and x_test are 3D arrays representing the images, where the first dimension is the number of images, the second and third dimensions are the height and width of each image, respectively. y_train and y_test are 1D arrays representing the corresponding labels (i.e. the digit that each image represents).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data to 4D tensor with shape (batch_size, height, width, channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "The reshape function is used to reshape the input data (x_train and x_test) from a 3D tensor of shape (num_samples, height, width) to a 4D tensor of shape (num_samples, height, width, channels), where channels is set to 1. This is necessary because TensorFlow requires input data to be in the form of a 4D tensor, where the first dimension is the batch size, the second and third dimensions are the height and width of each image, and the fourth dimension is the number of channels (1 for grayscale images, 3 for color images).\n",
    "\n",
    "The to_categorical function is used to convert the class labels (y_train and y_test) from integers to one-hot encoded vectors. This is necessary because deep learning models typically output a probability distribution over the classes, so the class labels must be represented as vectors of 0s and 1s. The to_categorical function from tensorflow.keras.utils takes the class labels as input and returns a binary matrix of shape (num_samples, num_classes), where num_samples is the number of samples and num_classes is the number of classes in the dataset. Each row of the matrix represents a sample, and the columns represent the classes, with a 1 indicating the sample's corresponding class and 0s for all other classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, activation='softmax'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "This code block builds a convolutional neural network (CNN) model using the Sequential class from tensorflow.keras.models. The model consists of the following layers:\n",
    "\n",
    "The first layer is a Conv2D layer with 32 filters, a kernel size of (3, 3), and a ReLU activation function. This layer takes input of shape (28, 28, 1), which corresponds to the height, width, and number of channels of the input image.\n",
    "\n",
    "The second layer is a MaxPooling2D layer with a pool size of (2, 2). This layer downsamples the output of the previous layer by taking the maximum value within a 2x2 pixel window.\n",
    "\n",
    "The third layer is another Conv2D layer with 64 filters, a kernel size of (3, 3), and a ReLU activation function.\n",
    "\n",
    "The fourth layer is another MaxPooling2D layer with a pool size of (2, 2).\n",
    "\n",
    "The fifth layer is a Flatten layer, which flattens the output of the previous layer into a 1D vector.\n",
    "\n",
    "The sixth layer is a Dense layer with 128 units and a ReLU activation function.\n",
    "\n",
    "The seventh layer is a Dropout layer with a rate of 0.5. This layer randomly sets 50% of the input units to 0 during training to prevent overfitting.\n",
    "\n",
    "The eighth layer is another Dense layer with 10 units and a softmax activation function. This layer outputs a probability distribution over the 10 classes (digits 0-9).\n",
    "\n",
    "Overall, this model performs two rounds of convolution and pooling to extract features from the input images, followed by two fully connected layers to classify the images. The dropout layer is used to prevent overfitting during training.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "The compile method is called on the model object with three arguments: optimizer, loss, and metrics.\n",
    "\n",
    "optimizer specifies the optimization algorithm used during training, which is Adam in this case.\n",
    "\n",
    "loss specifies the loss function used to measure the difference between the predicted and true labels. In this case, categorical_crossentropy is used as the loss function.\n",
    "\n",
    "metrics specifies the evaluation metric used during training and testing. Here, we are using accuracy as the evaluation metric.\n",
    "\n",
    "The fit method is called on the model object with the training and validation data, number of epochs, and batch size as arguments.\n",
    "\n",
    "epochs specifies the number of times the model should iterate over the entire training dataset.\n",
    "\n",
    "batch_size specifies the number of samples processed by the model in each iteration.\n",
    "\n",
    "The fit method returns a history object that contains information about the training and validation loss and accuracy over each epoch.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print training and testing accuracy\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Training accuracy:', train_acc)\n",
    "print('Testing accuracy:', test_acc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "The history object returned by the fit method contains the training and validation loss and accuracy over each epoch.\n",
    "\n",
    "history.history is a dictionary that contains the training and validation loss and accuracy values.\n",
    "\n",
    "history.history['accuracy'] returns the list of training accuracy values over each epoch.\n",
    "\n",
    "train_acc = history.history['accuracy'][-1] retrieves the last value of training accuracy.\n",
    "\n",
    "model.evaluate method computes the loss value and evaluation metric values for the test dataset.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0) assigns the test loss and accuracy values to the variables test_loss and test_acc.\n",
    "\n",
    "The print function is used to print the training and testing accuracy values.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select a random image from test set and predict its label\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rand_idx = random.randint(0, len(x_test)-1)\n",
    "rand_img = np.expand_dims(x_test[rand_idx], axis=0)\n",
    "pred_label = np.argmax(model.predict(rand_img))\n",
    "\n",
    "# Print predicted label and ground truth label for the random image\n",
    "print(\"Random image prediction:\")\n",
    "print(\"Predicted label:\", pred_label)\n",
    "print(\"Ground truth label:\", np.argmax(y_test[rand_idx]))\n",
    "\n",
    "plt.imshow(x_test[rand_idx], cmap='gray')\n",
    "plt.show()\n",
    "print('Input image:')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import random module is used to generate a random integer index to select an image from the test set.\n",
    "\n",
    "np.expand_dims method is used to expand the dimensions of the image array to match the input shape of the model.\n",
    "\n",
    "model.predict method is used to predict the label of the input image.\n",
    "\n",
    "np.argmax function is used to get the index of the class with the highest probability score.\n",
    "\n",
    "The predicted label and the ground truth label of the selected image are printed using the print function.\n",
    "\n",
    "The plt.imshow function is used to display the selected image.\n",
    "\n",
    "The plt.show function is used to show the plot.\n",
    "\n",
    "The last two lines of code show the text \"Input image:\" and a plot of the image. However, it seems like the corresponding plot is missing from the code block.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
